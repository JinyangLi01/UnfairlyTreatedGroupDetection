{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Yifan Guan \n",
    "https://github.com/yifanguan/DatabaseRepair/blob/master/minority_detection/pattern_count.py\n",
    "\n",
    "pattern_count is the same as my naive algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatternCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "13\n",
      "4\n",
      "3\n",
      "7\n",
      "0\n",
      "0\n",
      "2\n",
      "6\n",
      "13\n",
      "4\n",
      "3\n",
      "7\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Pattern Count algorithm based on Coverage paper; adapted into Yuval's pattern detection algorithm\n",
    "Bitvector count calculation\n",
    "\n",
    "each attribute value has one bitarray, total number of bitarrays = sum of cardinalities of all attributes\n",
    "length of each bitarray = # of different value combinations in a dataset\n",
    "assume pattern has not somehow been transformed to the form ([0-9a-zA-Z] \\ X)*          (i.e. currently supported maximum cardinality of an attribute is 10 + 26 + 25 = 61)\n",
    "assume datafile cannot read directly with pandas\n",
    "'''\n",
    "from bitarray import bitarray\n",
    "from bitarray.util import zeros\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PatternCounter:\n",
    "    '''\n",
    "    Create a counter for each set of data (i.e. train data, test data)\n",
    "    Create object, call parse_data(), then call pattern_count()\n",
    "    add support for non-encoded dataset; that is, to deal with any-arbitrary attribute value for each attribute\n",
    "    '''\n",
    "    def __init__(self, filename, selected_attrs_names=None, selected_attrs_id=None, encoded=True):\n",
    "        '''\n",
    "        filename: name of the data file\n",
    "        selected_attrs_names: selected attributes considered in a lattice\n",
    "        selected_attrs_id: id of selected attributes considered in a lattice\n",
    "        both of them are in a iterable format (i.e. list) At least one of them should be provided\n",
    "        encoded: whether the dataset has been processed\n",
    "        '''\n",
    "        self.filename = filename\n",
    "        self.selected_attrs_names = selected_attrs_names\n",
    "        self.selected_attrs_id = selected_attrs_id\n",
    "        self.encoded = encoded\n",
    "        self.num_attrs = -1\n",
    "        self.cardinalities = None\n",
    "        self.count_map = {} # map pattern (value combination) -> frequency in data\n",
    "        self.occurences = [] # values of count_map for faster calculation\n",
    "        self.dataBitVec = [] # list of bitarrays, each index of one bitarray corresponds to a\n",
    "        self.num_unique_value_combinations = -1\n",
    "        self.char_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "            'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A',\n",
    "            'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z']\n",
    "        self.attr_value_map = None # map of maps, [attr_index][attr_value] stores the index of one attribute value within one attribute\n",
    "\n",
    "    def parse_data(self):\n",
    "        # open file, read corresponding columns\n",
    "        df = pd.read_csv(self.filename, delimiter=', *', engine='python')\n",
    "        if self.selected_attrs_names is not None:\n",
    "            df = df[self.selected_attrs_names]\n",
    "            self.num_attrs = len(self.selected_attrs_names)\n",
    "        else:\n",
    "            df = df.iloc[:, self.selected_attrs_id]\n",
    "            self.num_attrs = len(self.selected_attrs_id)\n",
    "        data = df.values.astype(str) # 2d np array\n",
    "        cardinalities = list(df.nunique())\n",
    "        self.cardinalities = cardinalities\n",
    "\n",
    "        # actual mapping for non-encoded dataset to keep track of locations of attribute-value bitarrays\n",
    "        if not self.encoded:\n",
    "            self.attr_value_map = {}\n",
    "            for i in range(self.num_attrs):\n",
    "                self.attr_value_map[i] = {}\n",
    "                attr_values = np.unique(data[:,i])\n",
    "                for j, attr_value in enumerate(attr_values):\n",
    "                    self.attr_value_map[i][attr_value] = j\n",
    "\n",
    "        if not self.encoded:\n",
    "            for attr_value_list in data: # each row in data is a list containing attribute values\n",
    "                value_combination = '|'.join(attr_value_list) # use | to seperate different string values\n",
    "                if value_combination in self.count_map:\n",
    "                    self.count_map[value_combination] += 1\n",
    "                else:\n",
    "                    self.count_map[value_combination] = 1\n",
    "        else:\n",
    "            # construct count map to count frequency of each value combination in a dataset\n",
    "            for attr_value_list in data: # each row in data is a list containing attribute values\n",
    "                value_combination = ''.join(attr_value_list)\n",
    "                if value_combination in self.count_map:\n",
    "                    self.count_map[value_combination] += 1\n",
    "                else:\n",
    "                    self.count_map[value_combination] = 1\n",
    "\n",
    "        # create bitarrays\n",
    "        uniqueValueCombinations = list(self.count_map.keys())\n",
    "        for pattern in uniqueValueCombinations:\n",
    "            self.occurences.append(self.count_map[pattern])\n",
    "        self.num_unique_value_combinations = len(uniqueValueCombinations)\n",
    "\n",
    "        zero_bitarray = zeros(self.num_unique_value_combinations) # allocate a bitarray with length = # of unique value combinations\n",
    "        self.dataBitVec = [bitarray()] * sum(cardinalities)\n",
    "        self.dataBitVec[0] = zero_bitarray\n",
    "        for i in range(1, sum(cardinalities)):\n",
    "            self.dataBitVec[i] = bitarray(zero_bitarray) # this is fast deep copy based on the reference manual\n",
    "\n",
    "        if not self.encoded:\n",
    "            for i in range(self.num_unique_value_combinations):\n",
    "                value_combination = uniqueValueCombinations[i].split('|')\n",
    "                for j in range(self.num_attrs):\n",
    "                    self.dataBitVec[self.bitarray_index(value_combination[j], j)][i] = 1\n",
    "        else:\n",
    "            # fill bitarrays\n",
    "            for i in range(self.num_unique_value_combinations):\n",
    "                for j in range(self.num_attrs):\n",
    "                    self.dataBitVec[self.bitarray_index(uniqueValueCombinations[i][j], j)][i] = 1\n",
    "\n",
    "\n",
    "    def pattern_count(self, pattern):\n",
    "        '''\n",
    "        This is the function directly called by the search algorithm\n",
    "        pattern is of the format: 0X100X, etc\n",
    "        return the coverge/count of a pattern\n",
    "        '''\n",
    "        if not self.encoded: # dataset contains non-processed strings (i.e. not in the format of OXXX, etc)\n",
    "            and_bitarray = bitarray(self.num_unique_value_combinations)\n",
    "            and_bitarray.setall(1)\n",
    "            attr_values = pattern.split('|')\n",
    "            for i in range(self.num_attrs):\n",
    "                if attr_values[i] != '': # this attribute value is deterministic, not X in encoded case\n",
    "                    if attr_values[i] not in self.attr_value_map[i]: # if this attribute value is not shown/mentioned in a dataset, its count should be 0 (testcase 3)\n",
    "                        return 0\n",
    "                    and_bitarray = and_bitarray & self.dataBitVec[self.bitarray_index(attr_values[i], i)]       \n",
    "\n",
    "            count = 0\n",
    "            for i, bit in enumerate(and_bitarray):\n",
    "                if bit:\n",
    "                    count += self.occurences[i]\n",
    "            return count\n",
    "        else:\n",
    "            and_bitarray = bitarray(self.num_unique_value_combinations)\n",
    "            and_bitarray.setall(1)\n",
    "            for i in range(self.num_attrs):\n",
    "                if pattern[i] != 'X':\n",
    "                    and_bitarray = and_bitarray & self.dataBitVec[self.bitarray_index(pattern[i], i)]       \n",
    "\n",
    "            count = 0\n",
    "            for i, bit in enumerate(and_bitarray):\n",
    "                if bit:\n",
    "                    count += self.occurences[i]\n",
    "            return count\n",
    "\n",
    "\n",
    "    def char_index(self, character):\n",
    "        '''\n",
    "        This is a private function used to convert char to index\n",
    "        '''\n",
    "        return self.char_list.index(character)\n",
    "\n",
    "    def bitarray_index(self, attr_value, attr_index):\n",
    "        '''\n",
    "        can also dict to access bitarray; use attribute index and attribute value as keys\n",
    "        attr_value: value of an attribute. For character encoded data, attribute value is a single character. \n",
    "            For non-encoded data, attribute value is the orignal value\n",
    "        attr_index: index of the attribute in all attributes\n",
    "        '''\n",
    "        if not self.encoded:\n",
    "            return self.attr_value_map[attr_index][attr_value] + sum(self.cardinalities[:attr_index])\n",
    "        return self.char_index(attr_value) + sum(self.cardinalities[:attr_index])\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Test case to check for pattern count correctness\n",
    "    '''\n",
    "    pc = PatternCounter('test_data.txt', ['col1', 'col2', 'col3', 'col4'])\n",
    "    pc.parse_data()\n",
    "    print(pc.pattern_count('2XXX')) # 6\n",
    "    print(pc.pattern_count('XXXX')) # 13\n",
    "    print(pc.pattern_count('2001')) # 4\n",
    "    print(pc.pattern_count('X1XX')) # 3\n",
    "    print(pc.pattern_count('X0XX')) # 7\n",
    "    print(pc.pattern_count('X10X')) # 0\n",
    "    print(pc.pattern_count('21XX')) # 0\n",
    "    print(pc.pattern_count('X1X0')) # 2\n",
    "\n",
    "    pc2 = PatternCounter('test_data2.txt', ['col1', 'col2', 'col3', 'col4'], encoded=False)\n",
    "    pc2.parse_data()\n",
    "    print(pc2.pattern_count('hehe|||')) # 6\n",
    "    print(pc2.pattern_count('|||')) # 13\n",
    "    print(pc2.pattern_count('hehe|haha|0|Yifan Guan')) # 4\n",
    "    print(pc2.pattern_count('|123||')) # 3\n",
    "    print(pc2.pattern_count('|haha||')) # 7\n",
    "    print(pc2.pattern_count('|123|0|')) # 0\n",
    "    print(pc2.pattern_count('hehe|123||')) # 0\n",
    "    print(pc2.pattern_count('|123||Yifan')) # 2\n",
    "\n",
    "    pc3 = PatternCounter('CleanAdult4.csv', ['age', 'workclass', 'education', 'educational-num'], encoded=False)\n",
    "    pc3.parse_data()\n",
    "    print(pc3.pattern_count('|||37')) # 1\n",
    "    print(pc3.pattern_count('|||33')) # 0\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use pattern_count and my naive function for adult dataset\n",
    "error: pattern_count doesn't allows unseen pattern, have talked with Yifan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> ['col1', 'col2', 'col3', 'col4']\n"
     ]
    }
   ],
   "source": [
    "s = ['col1', 'col2', 'col3', 'col4']\n",
    "print(type(s), s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('M30.pkl', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    cands = pickle.load(filehandle)\n",
    "\n",
    "    \n",
    "data = pd.read_csv(\"CleanAdult3.csv\")\n",
    "dataarray = np.array(data)\n",
    "datalist = dataarray.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> ['age', 'workclass', 'education', 'educational-num', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n"
     ]
    }
   ],
   "source": [
    "column_list = np.array(data.columns).tolist()\n",
    "print(type(column_list), column_list) \n",
    "pc = PatternCounter('CleanAdult3.csv', column_list, encoded=False)\n",
    "pc.parse_data()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "mc, mcdes, attributes = Prepatation('miss_class3.csv')\n",
    "mcarray = np.array(mc)\n",
    "mclist = mcarray.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num2string(pattern):\n",
    "    st = ''\n",
    "    for i in pattern:\n",
    "        if i != -1:\n",
    "            st += str(i)\n",
    "        st += '|'\n",
    "    st = st[:-1]\n",
    "    return st\n",
    "\n",
    "\n",
    "\n",
    "def P1DominatedByP2(P1, P2):\n",
    "    length = len(P1)\n",
    "    for i in range(length):\n",
    "        if P1[i] == -1:\n",
    "            if P2[i] != -1:\n",
    "                return False\n",
    "        if P1[i] != -1:\n",
    "            if P2[i] != P1[i] and P2[i] != -1:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# whether a pattern P is dominated by MUP M\n",
    "def PDominatedByM(P, M):\n",
    "    for m in M:\n",
    "        if P1DominatedByP2(P, m):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# whether a pattern P dominates MUP M\n",
    "def PDominatesM(P, M):\n",
    "    for m in M:\n",
    "        if P1DominatedByP2(m, P):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# coverage of P among dataset D\n",
    "def cov(P, D):\n",
    "    cnt = 0\n",
    "    for d in D:\n",
    "        if P1DominatedByP2(d, P):\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# check for correctness of pattern_count and my naive algorithm\n",
    "for c in cands:\n",
    "    covdata = cov(c, datalist)\n",
    "    st = num2string(c)\n",
    "    #print(st, covdata)\n",
    "    patternCount = pc.pattern_count(st)\n",
    "    if covdata != patternCount:\n",
    "        print(c, covdata, patternCount, \"not equal\")\n",
    "print('end')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
